{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Overview\n",
    "Data preprocessing is critical to ensure the dataset is clean, consistent, and informative, maximizing the accuracy of the analytics we perform. This dataset's preprocessing will cover several essential steps: handling missing values, standardizing categorical data, extracting IP information, time formatting, and scaling. Each step aims to improve the quality of data for effective alerting and policy enforcement.\n",
    "\n",
    "### Handling Missing Values:\n",
    "Missing data, especially in critical fields, can compromise insights. For this step, we'll handle nulls appropriately based on column type and purpose.\n",
    "\n",
    "### Feature Extraction:\n",
    "Some columns, like Source and Target, contain JSON-like nested information. We'll extract relevant information, such as IPs and ports, to analyze traffic patterns and anomalies.\n",
    "\n",
    "### Encoding Categorical Variables:\n",
    "Categorical data like Category and Format need encoding so that the machine learning models can interpret them correctly.\n",
    "\n",
    "### DateTime Formatting:\n",
    "The DetectTime, WinStartTime, and WinEndTime columns contain timestamps. Properly formatting and aligning these will allow for time-based analyses, such as detecting unusual access times.\n",
    "\n",
    "### Scaling Numeric Features: \n",
    "Features such as ByteCount, PacketCount, and FlowCount have different ranges. Scaling these values can improve the model’s performance by giving equal importance to all numeric features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Importing Libaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/USER/UEBA_Project/alerting_policy_enforcement/data/raw/alert_policy_sample.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Handle Missing Values\n",
    "We’ll inspect the dataset to understand the extent of missing values and choose how to handle them based on their significance. Columns like ByteCount, PacketCount, and EventTime with many missing values may require imputation or removal, depending on their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " ByteCount       496660\n",
      "Category             0\n",
      "CeaseTime       215707\n",
      "ConnCount        75296\n",
      "CreateTime      264616\n",
      "DetectTime           0\n",
      "EventTime       211508\n",
      "FlowCount       264629\n",
      "Format               0\n",
      "ID                   0\n",
      "Node                 0\n",
      "PacketCount     496718\n",
      "Source            2007\n",
      "Target          172823\n",
      "WinEndTime      395413\n",
      "WinStartTime    395413\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7284\\2847865510.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['ConnCount'].fillna(df['ConnCount'].median(), inplace=True)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7284\\2847865510.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['FlowCount'].fillna(0, inplace=True)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7284\\2847865510.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['PacketCount'].fillna(df['PacketCount'].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Drop columns with a high percentage of missing data if they aren't critical\n",
    "df.drop(columns=['ByteCount', 'EventTime', 'CreateTime'], inplace=True)\n",
    "\n",
    "# Impute missing values in other columns if necessary\n",
    "df['ConnCount'].fillna(df['ConnCount'].median(), inplace=True)\n",
    "df['FlowCount'].fillna(0, inplace=True)\n",
    "df['PacketCount'].fillna(df['PacketCount'].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Convert Time Columns to DateTime Format\n",
    "Convert columns like DetectTime, WinEndTime, and WinStartTime to DateTime format for better time-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7284\\948374484.py:4: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7284\\948374484.py:4: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7284\\948374484.py:4: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  DetectTime WinEndTime WinStartTime\n",
      "0  2019-03-11 00:05:28+00:00        NaT          NaT\n",
      "1  2019-03-11 00:05:21+00:00        NaT          NaT\n",
      "2  2019-03-11 00:05:30+00:00        NaT          NaT\n",
      "3  2019-03-11 00:05:58+00:00        NaT          NaT\n",
      "4  2019-03-11 00:05:58+00:00        NaT          NaT\n"
     ]
    }
   ],
   "source": [
    "# Convert time columns to DateTime format\n",
    "time_columns = ['DetectTime', 'WinEndTime', 'WinStartTime']\n",
    "for col in time_columns:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Check for any issues in the conversion\n",
    "print(df[time_columns].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extract Features from Time Columns\n",
    "Using time-based columns, we can extract additional features like the hour, day, or week to capture temporal patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to datetime format\n",
    "df['DetectTime'] = pd.to_datetime(df['DetectTime'], errors='coerce')\n",
    "df['WinEndTime'] = pd.to_datetime(df['WinEndTime'], errors='coerce')\n",
    "df['WinStartTime'] = pd.to_datetime(df['WinStartTime'], errors='coerce')\n",
    "\n",
    "# Extract additional time-based features\n",
    "df['DetectHour'] = df['DetectTime'].dt.hour\n",
    "df['DetectDay'] = df['DetectTime'].dt.day\n",
    "df['WinDuration'] = (df['WinEndTime'] - df['WinStartTime']).dt.total_seconds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction from JSON-like Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define helper function to safely parse JSON-like data\n",
    "def extract_field(data, field, nested_field=None):\n",
    "    try:\n",
    "        if pd.notnull(data):\n",
    "            parsed_data = ast.literal_eval(data)\n",
    "            if isinstance(parsed_data, list) and len(parsed_data) > 0:\n",
    "                if nested_field and nested_field in parsed_data[0]:\n",
    "                    return parsed_data[0][nested_field][0] if field in parsed_data[0] else None\n",
    "                return parsed_data[0][field][0] if field in parsed_data[0] else None\n",
    "    except (ValueError, SyntaxError, IndexError, KeyError):\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# Extract fields from `Node`, `Source`, and `Target`\n",
    "df['Node_SW'] = df['Node'].apply(lambda x: extract_field(x, 'SW'))\n",
    "df['Node_Type'] = df['Node'].apply(lambda x: extract_field(x, 'Type'))\n",
    "df['Source_Proto'] = df['Source'].apply(lambda x: extract_field(x, 'Proto'))\n",
    "df['Source_Port'] = df['Source'].apply(lambda x: extract_field(x, 'Port'))\n",
    "df['Target_Proto'] = df['Target'].apply(lambda x: extract_field(x, 'Proto'))\n",
    "df['Target_Port'] = df['Target'].apply(lambda x: extract_field(x, 'Port'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Encode Categorical Variables\n",
    "For columns like Category and Format, we can use one-hot encoding or label encoding for machine learning compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['category_encoder.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "encoder = LabelEncoder()\n",
    "df['Category_encoded'] = encoder.fit_transform(df['Category'])\n",
    "joblib.dump(encoder, 'category_encoder.pkl')\n",
    "\n",
    "\n",
    "# Use one-hot encoding for columns with multiple categorical values\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "encoded_proto = ohe.fit_transform(df[['Source_Proto', 'Target_Proto']].fillna('missing'))\n",
    "encoded_proto_df = pd.DataFrame(encoded_proto, columns=ohe.get_feature_names_out(['Source_Proto', 'Target_Proto']))\n",
    "df = pd.concat([df.reset_index(drop=True), encoded_proto_df], axis=1)\n",
    "joblib.dump(ohe, 'proto_onehot_encoder.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Scale Numerical Features\n",
    "To standardize numerical columns, scaling them can improve model performance, especially for algorithms sensitive to feature ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "numerical_cols = ['ConnCount', 'FlowCount', 'PacketCount', 'WinDuration']\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "joblib.dump(scaler, 'scaler.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Label encoder and scaler saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the preprocessed DataFrame to a new CSV file\n",
    "processed_file_path = 'C:/Users/USER/UEBA_Project/alerting_policy_enforcement/data/processed/alert_policy_preprocessed.csv'\n",
    "df.to_csv(processed_file_path, index=False)\n",
    "\n",
    "print(\"Preprocessing complete. Label encoder and scaler saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Label encoder, one-hot encoder, and scaler saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "import joblib\n",
    "import ast\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/USER/UEBA_Project/alerting_policy_enforcement/data/raw/alert_policy_sample.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# **Step 1: Handle Missing Values**\n",
    "df.dropna(subset=['ID'], inplace=True)\n",
    "# Update to avoid chained assignment warning\n",
    "for col in ['ByteCount', 'ConnCount', 'FlowCount', 'PacketCount']:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# **Step 2: Feature Extraction from JSON-like Columns**\n",
    "# Define helper function to safely parse JSON-like data\n",
    "def extract_field(data, field, nested_field=None):\n",
    "    try:\n",
    "        if pd.notnull(data):\n",
    "            parsed_data = ast.literal_eval(data)\n",
    "            if isinstance(parsed_data, list) and len(parsed_data) > 0:\n",
    "                if nested_field and nested_field in parsed_data[0]:\n",
    "                    return parsed_data[0][nested_field][0] if field in parsed_data[0] else None\n",
    "                return parsed_data[0][field][0] if field in parsed_data[0] else None\n",
    "    except (ValueError, SyntaxError, IndexError, KeyError):\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# Extract fields from `Node`, `Source`, and `Target`\n",
    "df['Node_SW'] = df['Node'].apply(lambda x: extract_field(x, 'SW'))\n",
    "df['Node_Type'] = df['Node'].apply(lambda x: extract_field(x, 'Type'))\n",
    "df['Source_Proto'] = df['Source'].apply(lambda x: extract_field(x, 'Proto'))\n",
    "df['Source_Port'] = df['Source'].apply(lambda x: extract_field(x, 'Port'))\n",
    "df['Target_Proto'] = df['Target'].apply(lambda x: extract_field(x, 'Proto'))\n",
    "df['Target_Port'] = df['Target'].apply(lambda x: extract_field(x, 'Port'))\n",
    "\n",
    "# **Step 3: Encode Categorical Columns**\n",
    "# Encode 'Category' and save the encoder\n",
    "encoder = LabelEncoder()\n",
    "df['Category_encoded'] = encoder.fit_transform(df['Category'])\n",
    "joblib.dump(encoder, 'C:/Users/USER/UEBA_Project/alerting_policy_enforcement/data/processed/category_encoder.joblib')\n",
    "\n",
    "# Use one-hot encoding for columns with multiple categorical values\n",
    "ohe = OneHotEncoder(sparse_output=False)  # Update `sparse` to `sparse_output`\n",
    "encoded_proto = ohe.fit_transform(df[['Source_Proto', 'Target_Proto']].fillna('missing'))\n",
    "encoded_proto_df = pd.DataFrame(encoded_proto, columns=ohe.get_feature_names_out(['Source_Proto', 'Target_Proto']))\n",
    "df = pd.concat([df.reset_index(drop=True), encoded_proto_df], axis=1)\n",
    "joblib.dump(ohe, 'C:/Users/USER/UEBA_Project/alerting_policy_enforcement/data/processed/proto_onehot_encoder.joblib')\n",
    "\n",
    "# **Step 4: Scale Numerical Features**\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['ConnCount', 'FlowCount', 'PacketCount']\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "joblib.dump(scaler, 'C:/Users/USER/UEBA_Project/alerting_policy_enforcement/data/processed/scaler.joblib')\n",
    "\n",
    "# **Step 5: Save Preprocessed Data**\n",
    "processed_file_path = 'C:/Users/USER/UEBA_Project/alerting_policy_enforcement/data/processed/alert_policy_preprocessed.csv'\n",
    "df.to_csv(processed_file_path, index=False)\n",
    "\n",
    "print(\"Preprocessing complete. Label encoder, one-hot encoder, and scaler saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
